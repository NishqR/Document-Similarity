{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1341ef59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nishq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nishq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nishq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\nishq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nishq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- LOADING MODEL---------------------------- \n",
      "MODEL LOADED in 0.95 seconds\n",
      "Doing embeddings for article 0\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 1\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 2\n",
      "Cell took 0.01 seconds to run.\n",
      "Doing embeddings for article 3\n",
      "Cell took 0.01 seconds to run.\n",
      "Doing embeddings for article 4\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 5\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 6\n",
      "Cell took 0.01 seconds to run.\n",
      "Doing embeddings for article 7\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 8\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 9\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 10\n",
      "Cell took 0.01 seconds to run.\n",
      "Doing embeddings for article 11\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 12\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 13\n",
      "Cell took 0.01 seconds to run.\n",
      "Doing embeddings for article 14\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 15\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 16\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 17\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 18\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 19\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 20\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 21\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 22\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 23\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 24\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 25\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 26\n",
      "Cell took 0.01 seconds to run.\n",
      "Doing embeddings for article 27\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 28\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 29\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 30\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 31\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 32\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 33\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 34\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 35\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 36\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 37\n",
      "Cell took 0.01 seconds to run.\n",
      "Doing embeddings for article 38\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 39\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 40\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 41\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 42\n",
      "Cell took 0.01 seconds to run.\n",
      "Doing embeddings for article 43\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 44\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 45\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 46\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 47\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 48\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 49\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 50\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 51\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 52\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 53\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 54\n",
      "Cell took 0.01 seconds to run.\n",
      "Doing embeddings for article 55\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 56\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 57\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 58\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 59\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 60\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 61\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 62\n",
      "Cell took 0.01 seconds to run.\n",
      "Doing embeddings for article 63\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 64\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 65\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 66\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 67\n",
      "Cell took 0.00 seconds to run.\n",
      "Doing embeddings for article 68\n",
      "Cell took 0.00 seconds to run.\n",
      "Script took 0.02 minutes to run.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "from time import time\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from statistics import mean\n",
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import threading\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "\n",
    "#stop_words = stopwords.words('english')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def preprocess(sentence):\n",
    "    return [w for w in sentence.lower().split() if w not in stop_words]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    run_relevant = True\n",
    "    main_start = time() \n",
    "\n",
    "    articles_df = pd.read_csv(\"all_articles.csv\")\n",
    "\n",
    "    if run_relevant == True:\n",
    "\n",
    "        articles = list(articles_df[articles_df['relevant_campbell'] == 1]['text'])\n",
    "\n",
    "    else:\n",
    "\n",
    "        articles = list(articles_df[articles_df['relevant_campbell'] == 0]['text'])        \n",
    "\n",
    "    \n",
    "\n",
    "    count = 0\n",
    "    print(\"---------------------------- LOADING MODEL---------------------------- \")\n",
    "    start = time()\n",
    "    global model\n",
    "    # This will download and load the pretrained model offered by UKPLab.\n",
    "    model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    \n",
    "    print('MODEL LOADED in %.2f seconds' % (time() - start))\n",
    "    \n",
    "    embeddings_dict = {}\n",
    "    count_dict = {}\n",
    "    \n",
    "    for article in articles:\n",
    "        \n",
    "        print(f\"Doing embeddings for article {count}\")\n",
    "        \n",
    "        start = time()\n",
    "        \n",
    "        sentences = sent_tokenize(article)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            words_in_sentence = list(sentence.split(\" \"))\n",
    "            for word_ in words_in_sentence:\n",
    "\n",
    "                word_ = word_.lower()\n",
    "                word_ = word_.strip()\n",
    "                word_ = word_.replace(\" \", \"\")\n",
    "                word_ = word_.replace(\",\", \"\")\n",
    "                word_ = word_.replace(\".\", \"\")\n",
    "                word_ = word_.replace(\":\", \"\")\n",
    "\n",
    "                if lemmatizer.lemmatize(word_) != 'ha' or lemmatizer.lemmatize(word_) != 'wa':\n",
    "                    word_ = lemmatizer.lemmatize(word_)\n",
    "\n",
    "                if word_ not in stop_words and word_ not in string.punctuation:\n",
    "                    if word_ not in embeddings_dict.keys():\n",
    "                        #embeddings_dict[word_] = model.encode(word_)\n",
    "                        embeddings_dict[word_] = \"\"\n",
    "                        count_dict[word_] = 1\n",
    "\n",
    "                    else:\n",
    "                        count_dict[word_] += 1\n",
    "\n",
    "        count+=1\n",
    "        print('Cell took %.2f seconds to run.' % (time() - start))\n",
    "\n",
    "    #print(sorted(((v, k) for k, v in count_dict.items()), reverse=True))\n",
    "    output_words = pd.Series(count_dict)\n",
    "    output_df = pd.DataFrame(pd.Series(count_dict))\n",
    "    '''\n",
    "    if run_relevant == True:\n",
    "        output_df.sort_values(by=[0], ascending=False).to_csv(\"relevant_words_count.csv\")\n",
    "        \n",
    "    else:\n",
    "        output_df.sort_values(by=[0], ascending=False).to_csv(\"irrelevant_words_count.csv\")\n",
    "    '''\n",
    "    print('Script took %.2f minutes to run.' % ((time() - main_start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "849b84fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['senator', 'reverend', 'warnock', 'introduces', 'bill', 'ensure',\n",
       "       'georgia', 'car', 'buyer', 'automaker',\n",
       "       ...\n",
       "       'qe', 'quickly\"', 'fondness', 'sailboat', 'boat', 'honeymoon', 'pact',\n",
       "       'inspired', 'sea', 'million?'],\n",
       "      dtype='object', length=5392)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_words.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7cef04bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pension', 'said', 'retirement', 'year', 'loan', 'mortgage', 'money',\n",
       "       'rate', 'student', 'pay', 'ha', 'people', 'plan', 'tax', 'debt', 'also',\n",
       "       'per', 'say', 'financial', 'new'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(output_df.sort_values(by=[0], ascending=False).head(20).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9307e8d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'pension'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13864/1686401892.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moutput_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5485\u001b[0m         ):\n\u001b[0;32m   5486\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5487\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5489\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'pension'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7775219a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
